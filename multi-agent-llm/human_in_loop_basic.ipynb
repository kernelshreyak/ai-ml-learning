{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4da470-21d7-4730-8f46-dc1e53e8a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d551d42-3e09-42c1-9e45-5b442eeee974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemoryError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8290c00c-cd88-4d35-a2df-aebee20d27bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "\"\"\"\n",
    "In previous examples we've annotated the `messages` state key\n",
    "with the default `operator.add` or `+` reducer, which always\n",
    "appends new messages to the end of the existing messages array.\n",
    "\n",
    "Now, to support replacing existing messages, we annotate the\n",
    "`messages` key with a customer reducer function, which replaces\n",
    "messages with the same `id`, and appends them otherwise.\n",
    "\"\"\"\n",
    "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list[AnyMessage]:\n",
    "    # assign ids to messages that don't have them\n",
    "    for message in right:\n",
    "        if not message.id:\n",
    "            message.id = str(uuid4())\n",
    "    # merge the new messages with the existing messages\n",
    "    merged = left.copy()\n",
    "    for message in right:\n",
    "        for i, existing in enumerate(merged):\n",
    "            # replace any existing messages with the same id\n",
    "            if existing.id == message.id:\n",
    "                merged[i] = message\n",
    "                break\n",
    "        else:\n",
    "            # append any new messages to the end\n",
    "            merged.append(message)\n",
    "    return merged\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], reduce_messages]tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847a42c4-d483-4038-a79d-5689baca05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef0b6b8-a279-4b81-b0a2-9c6734d0a8ec",
   "metadata": {},
   "source": [
    "### Manual human approval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f499cd4a-dbb9-4cb1-bac7-2c0f2fc2a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer,\n",
    "            interrupt_before=[\"action\"]\n",
    "        )\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        print(state)\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0149bab-01e6-409f-bd25-d923046e4668",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
