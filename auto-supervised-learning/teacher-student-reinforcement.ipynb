{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import layers, Model, losses, optimizers\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: CHANGE THIS FOR NEW TASK\n",
    "TASK_ID = \"classification_movie_sentiment_1\"\n",
    "OBJECTIVE = \"binary sentiment classification of movie reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_memory(df_memory: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Persist the DataFrame to a .npy file as a structured (record) array.\n",
    "\n",
    "    Args:\n",
    "      df_memory: The DataFrame containing columns ['text','label','cycle','split'] (or similar).\n",
    "      filepath:   Path where to write the .npy file.\n",
    "    \"\"\"\n",
    "    records = df_memory.to_records(index=False)\n",
    "    filepath = f\"{TASK_ID}.npy\"\n",
    "    np.save(filepath, records)\n",
    "    print(f\"Memory saved to {filepath}\")\n",
    "\n",
    "def load_memory() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a previously saved memory .npy file back into a DataFrame.\n",
    "\n",
    "    Args:\n",
    "      filepath: Path to the .npy file created by save_memory.\n",
    "\n",
    "    Returns:\n",
    "      A pandas DataFrame reconstructed from the record array.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filepath = f\"{TASK_ID}.npy\"\n",
    "        records = np.load(filepath, allow_pickle=True)\n",
    "        return pd.DataFrame.from_records(records)\n",
    "    except:\n",
    "        return pd.DataFrame(columns=[\"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text label\n",
      "0   I absolutely loved this movie, it was fantasti...     1\n",
      "1   The film was a complete waste of time, very bo...     0\n",
      "2   An outstanding performance by the lead actor, ...     1\n",
      "3   I didn't enjoy the plot; it felt disjointed an...     0\n",
      "4   A beautiful story with excellent cinematograph...     1\n",
      "5   Terrible acting and a storyline that made no s...     0\n",
      "6   The soundtrack was amazing and really helped s...     1\n",
      "7    I fell asleep halfway through; it was that dull.     0\n",
      "8   Such a heartwarming film, I would watch it again.     1\n",
      "9   The dialogues were cheesy and the characters w...     0\n",
      "10  This movie exceeded all my expectations, highl...     1\n",
      "11  Poorly directed with many plot holes, not wort...     0\n",
      "12  A brilliant script and superb acting made this...     1\n",
      "13  I couldn't connect with the story or the chara...     0\n",
      "14        Visually stunning and emotionally powerful.     1\n",
      "15  The pacing was slow and the ending was disappo...     0\n",
      "16   An inspiring tale that left me feeling uplifted.     1\n",
      "17  Lackluster and forgettable, I wouldn't recomme...     0\n",
      "18        The humor was clever and the plot engaging.     1\n",
      "19  I regret watching this movie; it was a total f...     0\n",
      "(80, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_memory.head(20))\n",
    "print(df_memory.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Teacher (LLM) Setup ───────────────────────────────────────────────────────\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  # alias for openai_api_key :contentReference[oaicite:1]{index=1}\n",
    "    model=\"gpt-4.1-mini\",                        # use 'model' instead of model_name :contentReference[oaicite:2]{index=2}\n",
    "    temperature=0.7,\n",
    "    max_retries=2\n",
    ")\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"objective\", \"n_samples\"],\n",
    "    template=(\n",
    "        \"You are a data generator for the NLP task: {objective}.\\n\"\n",
    "        \"Generate exactly {n_samples} JSON objects in a list, each with:\\n\"\n",
    "        \"  - \\\"text\\\": a short example input\\n\"\n",
    "        \"  - \\\"label\\\": the correct output for that input. For classification task, the label is a number defined in the objective \\n\"\n",
    "        \"Return only valid JSON.\"\n",
    "    )\n",
    ")\n",
    "generator_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "def cleanup_json(input_jsonstring: str):\n",
    "    return input_jsonstring.replace(\"`\",\"\").replace(\"json\",\"\")\n",
    "\n",
    "def generate_labels(resp,task_type:str):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    examples = json.loads(resp)\n",
    "    if task_type == \"text-classification\":\n",
    "        texts = [ex[\"text\"] for ex in examples]\n",
    "        labels = [int(ex[\"label\"]) for ex in examples]\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# ─── Refactored generate_examples ──────────────────────────────────────────────\n",
    "def generate_examples(objective: str, n_samples: int):\n",
    "    \"\"\"\n",
    "    1) Ask the Teacher LLM for n_samples examples,\n",
    "    2) append them into the global df_memory,\n",
    "    3) persist to disk,\n",
    "    4) return texts & labels.\n",
    "    \"\"\"\n",
    "    resp = generator_chain.run({\"objective\": objective, \"n_samples\": n_samples})\n",
    "    resp = cleanup_json(resp)\n",
    "    texts,labels = generate_labels(resp,\"text-classification\")\n",
    "\n",
    "    # update in‐memory DataFrame and persist\n",
    "    global df_memory\n",
    "    df_new = pd.DataFrame({\"text\": texts, \"label\": labels})\n",
    "    df_memory = pd.concat([df_memory, df_new], ignore_index=True)\n",
    "    save_memory(df_memory)\n",
    "    print(\"Memory updated\")\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# ─── New train/test split function ─────────────────────────────────────────────\n",
    "def get_train_test(test_size: float = 0.2,\n",
    "                   shuffle: bool = True,\n",
    "                   random_state: int = None):\n",
    "    \"\"\"\n",
    "    Load the full df_memory from disk, then randomly split into\n",
    "    train/test DataFrames according to test_size fraction.\n",
    "    \"\"\"\n",
    "    df = load_memory()\n",
    "    if shuffle:\n",
    "        df = df.sample(frac=1.0, random_state=random_state).reset_index(drop=True)\n",
    "    n_test = int(len(df) * test_size)\n",
    "    df_test  = df.iloc[:n_test].reset_index(drop=True)\n",
    "    df_train = df.iloc[n_test:].reset_index(drop=True)\n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Student (RNN) & Preprocessing ─────────────────────────────────────────────\n",
    "\n",
    "max_len    = 40\n",
    "vocab_size = 10000\n",
    "embed_dim  = 128\n",
    "lstm_units = 128\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_len\n",
    ")\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(\n",
    "    package=\"Custom\", name=\"StudentModel\"\n",
    ")\n",
    "class StudentModel(Model):\n",
    "    # TODO: need to make this architecture custom so that it can utilize reinforcement learning supervised by the teacher model\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          num_classes: number of output classes\n",
    "          **kwargs: passed to keras.Model (e.g. trainable, dtype)\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Define your layers as before\n",
    "        self.embedding  = layers.Embedding(input_dim=vocab_size,\n",
    "                                           output_dim=embed_dim)\n",
    "        self.bilstm     = layers.Bidirectional(\n",
    "                            layers.LSTM(lstm_units)\n",
    "                        )\n",
    "        self.dense1     = layers.Dense(64, activation='relu')\n",
    "        self.classifier = layers.Dense(num_classes,\n",
    "                                       activation='softmax')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = vectorize_layer(inputs)\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Return a serializable config dict for this model.\n",
    "        Must include all __init__ args.\n",
    "        \"\"\"\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"num_classes\": self.num_classes}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"\n",
    "        Reconstruct the model from its config.\n",
    "        \"\"\"\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Curriculum Loop ──────────────────────────────────────────────────────────\n",
    "\n",
    "def curriculum_loop(\n",
    "    objective: str,\n",
    "    num_cycles: int = 5,\n",
    "    N_train: int = 100,\n",
    "    N_test: int = 50,\n",
    "    batch_size: int = 16,\n",
    "    lambda_punish: float = 0.5\n",
    "):\n",
    "    # Bootstrap: generate initial data and adapt vectorizer\n",
    "    init_texts, init_labels = generate_examples(objective, N_train)\n",
    "    print(\"[INIT BOOTSTRAP]: Examples generated\")\n",
    "    vectorize_layer.adapt(\n",
    "        tf.data.Dataset.from_tensor_slices(init_texts).batch(batch_size)\n",
    "    )\n",
    "    \n",
    "    num_classes = len(set(init_labels))\n",
    "    print(\"[INIT BOOTSTRAP]: num_classes = \",num_classes)\n",
    "    # Initialize Student model\n",
    "    student = StudentModel(num_classes)\n",
    "    student.compile(\n",
    "        optimizer=optimizers.Adam(1e-3),\n",
    "        loss=losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    print(\"[INIT BOOTSTRAP]: student model initialized\")\n",
    "    \n",
    "    prev_score = 0.0\n",
    "\n",
    "    for cycle in range(1, num_cycles + 1):\n",
    "        print(f\"\\n=== Cycle {cycle} ===\")\n",
    "\n",
    "        # a) Augment memory with N_train new examples\n",
    "        generate_examples(objective, N_train)\n",
    "        print(f\"[CYCLE {cycle}]: Generated {N_train} new examples; total memory = {len(df_memory)}\")\n",
    "\n",
    "        # b) Randomly split the full memory into train & test\n",
    "        #    Here we aim for N_test examples, so compute fraction.\n",
    "        test_frac = N_test / len(df_memory)\n",
    "        df_train, df_test = get_train_test(test_size=test_frac, random_state=42)\n",
    "\n",
    "        train_texts = df_train[\"text\"].to_numpy()\n",
    "        train_labels = df_train[\"label\"].astype(np.int32).to_numpy()\n",
    "        test_texts  = df_test[\"text\"].to_numpy()\n",
    "        test_labels  = df_test[\"label\"].astype(np.int32).to_numpy()\n",
    "\n",
    "\n",
    "        # c) Build tf.data pipelines from the DataFrames\n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((\n",
    "            tf.constant(train_texts, dtype=tf.string),\n",
    "            tf.constant(train_labels, dtype=tf.int32),\n",
    "        )).batch(batch_size)\n",
    "\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((\n",
    "            tf.constant(test_texts, dtype=tf.string),\n",
    "            tf.constant(test_labels, dtype=tf.int32),\n",
    "        )).batch(batch_size)\n",
    "\n",
    "        # d) Train for a few epochs\n",
    "        student.fit(train_ds, epochs=5, verbose=1)\n",
    "\n",
    "        # e) Evaluate on the held-out split\n",
    "        loss, acc = student.evaluate(test_ds, verbose=0)\n",
    "        score = float(acc)\n",
    "        print(f\"Test accuracy: {score:.4f}\")\n",
    "\n",
    "        # f) Punishment term (if score dropped)\n",
    "        punish = max(0.0, prev_score - score)\n",
    "        if punish > 0:\n",
    "            print(f\"Punishment λ·Δ = {lambda_punish * punish:.4f}\")\n",
    "\n",
    "        prev_score = score\n",
    "\n",
    "    return student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the curriculum loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory saved to classification_movie_sentiment_1.npy\n",
      "Memory updated\n",
      "[INIT BOOTSTRAP]: Examples generated\n",
      "[INIT BOOTSTRAP]: num_classes =  2\n",
      "[INIT BOOTSTRAP]: student model initialized\n",
      "\n",
      "=== Cycle 1 ===\n",
      "Memory saved to classification_movie_sentiment_1.npy\n",
      "Memory updated\n",
      "[CYCLE 1]: Generated 20 new examples; total memory = 40\n",
      "Epoch 1/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.3042 - loss: 0.7011\n",
      "Epoch 2/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5625 - loss: 0.6864\n",
      "Epoch 3/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7597 - loss: 0.6809\n",
      "Epoch 4/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8028 - loss: 0.6751\n",
      "Epoch 5/5\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9125 - loss: 0.6655\n",
      "Test accuracy: 0.5000\n",
      "\n",
      "=== Cycle 2 ===\n",
      "Memory saved to classification_movie_sentiment_1.npy\n",
      "Memory updated\n",
      "[CYCLE 2]: Generated 20 new examples; total memory = 60\n",
      "Epoch 1/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7651 - loss: 0.6608\n",
      "Epoch 2/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8158 - loss: 0.6223\n",
      "Epoch 3/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.8527 - loss: 0.5674\n",
      "Epoch 4/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8158 - loss: 0.4817\n",
      "Epoch 5/5\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8527 - loss: 0.4089\n",
      "Test accuracy: 0.7000\n",
      "\n",
      "=== Cycle 3 ===\n",
      "Memory saved to classification_movie_sentiment_1.npy\n",
      "Memory updated\n",
      "[CYCLE 3]: Generated 20 new examples; total memory = 80\n",
      "Epoch 1/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8521 - loss: 0.4019\n",
      "Epoch 2/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8738 - loss: 0.2657\n",
      "Epoch 3/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.9132 - loss: 0.1918\n",
      "Epoch 4/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.9926 - loss: 0.0963\n",
      "Epoch 5/5\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 1.0000 - loss: 0.0482\n",
      "Test accuracy: 0.8000\n",
      "Done. Model saved to student_saved_model_tf2/\n"
     ]
    }
   ],
   "source": [
    "# ─── Execute Loop ─────────────────────────────────────────────────────────────\n",
    "df_memory = load_memory()\n",
    "if __name__ == \"__main__\":\n",
    "    final_student = curriculum_loop(\n",
    "        objective=OBJECTIVE,\n",
    "        num_cycles=3,\n",
    "        N_train=20,\n",
    "        N_test=10\n",
    "    )\n",
    "    final_student.save(\"student_saved_model_tf2.keras\")\n",
    "    print(\"Done. Model saved to student_saved_model_tf2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 270ms/step\n",
      "Probabilities: [0.7978721  0.20212786]\n",
      "Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = tf.keras.models.load_model(\"student_saved_model_tf2.keras\")\n",
    "\n",
    "# single example as tf.constant of dtype string\n",
    "sample_text = tf.constant([\"It was awesome\"])\n",
    "\n",
    "# now predict\n",
    "probs = model.predict(sample_text)    # shape (1, num_classes)\n",
    "pred_idx = tf.argmax(probs, axis=1).numpy()[0]\n",
    "\n",
    "print(\"Probabilities:\", probs[0])\n",
    "print(\"Predicted label:\", pred_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
