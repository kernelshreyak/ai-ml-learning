{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:08:03.576712: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-20 08:08:03.594510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740038883.616099   12273 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740038883.622785   12273 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-20 08:08:03.644948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/shreyak_rekshda/ai-ml-test/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import py7zr\n",
    "import time\n",
    "\n",
    "import keras_hub\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_BATCHES = 600\n",
    "EPOCHS = 1  # Can be set to a higher value for better results\n",
    "MAX_ENCODER_SEQUENCE_LENGTH = 512\n",
    "MAX_DECODER_SEQUENCE_LENGTH = 128\n",
    "MAX_GENERATION_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740038891.173901   12273 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20750 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset.\n",
    "filename = keras.utils.get_file(\n",
    "    \"corpus.7z\",\n",
    "    origin=\"https://huggingface.co/datasets/samsum/resolve/main/data/corpus.7z\",\n",
    ")\n",
    "\n",
    "# Extract the `.7z` file.\n",
    "with py7zr.SevenZipFile(filename, mode=\"r\") as z:\n",
    "    z.extractall(path=\"/home/shreyak_rekshda/tensorflow_datasets/downloads/manual\")\n",
    "\n",
    "# Load data using TFDS.\n",
    "samsum_ds = tfds.load(\"samsum\", split=\"train\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Carter: Hey Alexis, I just wanted to let you know that I had a really nice time with you tonight. \\r\\nAlexis: Thanks Carter. Yeah, I really enjoyed myself as well. \\r\\nCarter: If you are up for it, I would really like to see you again soon.\\r\\nAlexis: Thanks Carter, I'm flattered. But I have a really busy week coming up.\\r\\nCarter: Yeah, no worries. I totally understand. But if you ever want to go grab dinner again, just let me know. \\r\\nAlexis: Yeah of course. Thanks again for tonight. \\r\\nCarter: Sure. Have a great night. \"\n",
      "b'Alexis and Carter met tonight. Carter would like to meet again, but Alexis is busy.'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:08:39.528512: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-02-20 08:08:39.534358: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for dialogue, summary in samsum_ds:\n",
    "    print(dialogue.numpy())\n",
    "    print(summary.numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    samsum_ds.map(\n",
    "        lambda dialogue, summary: {\"encoder_text\": dialogue, \"decoder_text\": summary}\n",
    "    )\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    ")\n",
    "train_ds = train_ds.take(NUM_BATCHES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bart/keras/bart_base_en/2/download/config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483/483 [00:00<00:00, 830kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bart/keras/bart_base_en/2/download/tokenizer.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 448/448 [00:00<00:00, 713kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bart/keras/bart_base_en/2/download/assets/tokenizer/vocabulary.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 0.99M/0.99M [00:01<00:00, 593kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bart/keras/bart_base_en/2/download/assets/tokenizer/merges.txt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 446k/446k [00:01<00:00, 333kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/keras/bart/keras/bart_base_en/2/download/model.weights.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 532M/532M [00:44<00:00, 12.5MB/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"bart_seq2_seq_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"bart_seq2_seq_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ bart_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BartTokenizer</span>)                                │                       Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">50,265</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ bart_tokenizer (\u001b[38;5;33mBartTokenizer\u001b[0m)                                │                       Vocab size: \u001b[38;5;34m50,265\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"bart_seq2_seq_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"bart_seq2_seq_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_padding_mask          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_token_ids             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_padding_mask          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_token_ids             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bart_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BartBackbone</span>)  │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>),       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">139,417,344</span> │ decoder_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)]        │                 │ decoder_token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                               │                           │                 │ encoder_padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                               │                           │                 │ encoder_token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50265</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">38,603,520</span> │ bart_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ decoder_padding_mask          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_token_ids             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_padding_mask          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_token_ids             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ bart_backbone (\u001b[38;5;33mBartBackbone\u001b[0m)  │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m),       │     \u001b[38;5;34m139,417,344\u001b[0m │ decoder_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)]        │                 │ decoder_token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                               │                           │                 │ encoder_padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                               │                           │                 │ encoder_token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50265\u001b[0m)       │      \u001b[38;5;34m38,603,520\u001b[0m │ bart_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139,417,344</span> (531.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m139,417,344\u001b[0m (531.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">139,417,344</span> (531.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m139,417,344\u001b[0m (531.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preprocessor = keras_hub.models.BartSeq2SeqLMPreprocessor.from_preset(\n",
    "    \"bart_base_en\",\n",
    "    encoder_sequence_length=MAX_ENCODER_SEQUENCE_LENGTH,\n",
    "    decoder_sequence_length=MAX_DECODER_SEQUENCE_LENGTH,\n",
    ")\n",
    "bart_lm = keras_hub.models.BartSeq2SeqLM.from_preset(\n",
    "    \"bart_base_en\", preprocessor=preprocessor\n",
    ")\n",
    "\n",
    "bart_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    epsilon=1e-6,\n",
    "    global_clipnorm=1.0,  # Gradient clipping.\n",
    ")\n",
    "# Exclude layernorm and bias terms from weight decay.\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"bias\"])\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"gamma\"])\n",
    "optimizer.exclude_from_weight_decay(var_names=[\"beta\"])\n",
    "\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "bart_lm.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    weighted_metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:10:34.944831: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1740039078.350651   12447 service.cc:148] XLA service 0x7fbdb4079ff0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1740039078.351135   12447 service.cc:156]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
      "2025-02-20 08:11:19.930313: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1740039082.284665   12447 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1740039084.969055   12447 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-02-20 08:11:33.791648: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_381', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:34.495684: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_381', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:34.791381: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_381', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:34.888109: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_381_0', 344 bytes spill stores, 316 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:35.611621: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_381', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:35.702067: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_381', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:40.136563: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:40.565097: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:40.993807: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 344 bytes spill stores, 316 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:43.293960: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:44.101225: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:45.163725: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_358', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:48.436568: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:48.664095: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:49.990389: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 652 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:50.942213: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:51.151769: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100_0', 1148 bytes spill stores, 1400 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:52.328127: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100_0', 232 bytes spill stores, 232 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:52.673141: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 320 bytes spill stores, 320 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:52.897238: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_98', 320 bytes spill stores, 320 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:55.346588: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:57.094322: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_100', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:58.721055: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102_0', 1504 bytes spill stores, 1708 bytes spill loads\n",
      "\n",
      "2025-02-20 08:11:59.245633: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:02.320772: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_102', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:06.484683: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 652 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:06.700908: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 320 bytes spill stores, 320 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:07.341270: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:09.120332: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 236 bytes spill stores, 236 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:09.590173: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_154', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:10.522801: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 320 bytes spill stores, 320 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:11.034092: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 652 bytes spill stores, 616 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:12.362412: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:14.379118: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_478', 472 bytes spill stores, 476 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:14.492171: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:15.496232: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_478', 588 bytes spill stores, 588 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:16.757416: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_478', 112 bytes spill stores, 112 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:19.543626: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_156', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:20.771991: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_479', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:21.496404: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46668', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:21.831897: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_478', 2780 bytes spill stores, 2780 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:22.405640: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_158', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:23.906602: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_158', 380 bytes spill stores, 352 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:24.171536: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46668', 392 bytes spill stores, 392 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:24.957146: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46331', 132 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:24.961919: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46668', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:25.541060: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_158', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:26.615213: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46668', 10492 bytes spill stores, 10832 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:29.141036: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_213', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:31.873705: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46331', 468 bytes spill stores, 468 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:32.994236: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46668', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:36.719967: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_42928', 10588 bytes spill stores, 10968 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:37.448847: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46331', 10780 bytes spill stores, 11224 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:37.688668: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_47605', 856 bytes spill stores, 648 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:38.385155: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46331', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:39.164645: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_46331', 312 bytes spill stores, 312 bytes spill loads\n",
      "\n",
      "2025-02-20 08:12:42.505129: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_213', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-02-20 08:13:22.620867: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'copy_fusion_5', 596 bytes spill stores, 596 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 52 bytes spill stores, 52 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'copy_fusion_2', 464 bytes spill stores, 464 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function '__cuda_sm3x_div_rn_noftz_f32_slowpath', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1740039203.431583   12447 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m595/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 216ms/step - accuracy: 0.5536 - loss: 0.4341"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:15:31.903024: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-20 08:15:31.905926: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m600/600\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 221ms/step - accuracy: 0.5538 - loss: 0.4337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fbe40142f20>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_lm.fit(train_ds, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:16:04.671912: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-20 08:16:04.672906: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-02-20 08:16:12.867437: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_246_0', 124 bytes spill stores, 132 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:12.875065: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_246', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:13.935660: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_246', 340 bytes spill stores, 308 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:14.620573: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_246_0', 244 bytes spill stores, 244 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:16.459672: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_248', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:17.702341: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_248_0', 1424 bytes spill stores, 1620 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:17.891879: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_248', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:17.995721: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_248', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:18.886910: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_248', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:19.021534: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_248', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:29.006674: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_278', 340 bytes spill stores, 308 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:33.327334: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_278', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:16:33.460003: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_278_0', 116 bytes spill stores, 132 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Elapsed: 35.54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 08:16:50.826157: W tensorflow/core/kernels/data/cache_dataset_ops.cc:914] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-02-20 08:16:54.027881: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-02-20 08:17:01.059667: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:01.465685: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:06.580490: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339_0', 1548 bytes spill stores, 1804 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:07.448323: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371_0', 1556 bytes spill stores, 1804 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:08.247395: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:09.370839: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:11.271144: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:11.435739: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:29.017125: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:31.659435: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339_0', 1548 bytes spill stores, 1804 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:32.500778: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_339', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:33.993001: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:35.429216: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:37.149528: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 344 bytes spill stores, 316 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:38.653097: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:39.706709: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:40.582123: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_341', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:41.546783: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371_0', 1556 bytes spill stores, 1804 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:42.300598: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-02-20 08:17:44.058401: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_371', 288 bytes spill stores, 288 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time Elapsed: 70.52s\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, input_text, max_length=200, print_time_taken=False):\n",
    "    start = time.time()\n",
    "    output = model.generate(input_text, max_length=max_length)\n",
    "    end = time.time()\n",
    "    print(f\"Total Time Elapsed: {end - start:.2f}s\")\n",
    "    return output\n",
    "\n",
    "\n",
    "# Load the dataset.\n",
    "val_ds = tfds.load(\"samsum\", split=\"validation\", as_supervised=True)\n",
    "val_ds = val_ds.take(100)\n",
    "\n",
    "dialogues = []\n",
    "ground_truth_summaries = []\n",
    "for dialogue, summary in val_ds:\n",
    "    dialogues.append(dialogue.numpy())\n",
    "    ground_truth_summaries.append(summary.numpy())\n",
    "\n",
    "# Let's make a dummy call - the first call to XLA generally takes a bit longer.\n",
    "_ = generate_text(bart_lm, \"sample text\", max_length=MAX_GENERATION_LENGTH)\n",
    "\n",
    "# Generate summaries.\n",
    "generated_summaries = generate_text(\n",
    "    bart_lm,\n",
    "    val_ds.map(lambda dialogue, _: dialogue).batch(8),\n",
    "    max_length=MAX_GENERATION_LENGTH,\n",
    "    print_time_taken=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: b'Tony: Is the boss in?\\r\\nClaire: Not yet.\\r\\nTony: Could let me know when he comes, please? \\r\\nClaire: Of course.\\r\\nTony: Thank you.'\n",
      "Generated Summary: Tony is afraid the boss is in. Tony will let Claire know when he comes.\n",
      "Ground Truth Summary: b\"The boss isn't in yet. Claire will let Tony know when he comes.\"\n",
      "=============================\n",
      "Dialogue: b\"James: What shouldl I get her?\\r\\nTim: who?\\r\\nJames: gees Mary my girlfirend\\r\\nTim: Am I really the person you should be asking?\\r\\nJames: oh come on it's her birthday on Sat\\r\\nTim: ask Sandy\\r\\nTim: I honestly am not the right person to ask this\\r\\nJames: ugh fine!\"\n",
      "Generated Summary: James wants his girlfriend Mary to get birthday on Friday.\n",
      "Ground Truth Summary: b\"Mary's birthday is on Saturday. Her boyfriend, James, is looking for gift ideas. Tim suggests that he ask Sandy.\"\n",
      "=============================\n",
      "Dialogue: b\"Mary: So, how's Israel? Have you been on the beach?\\r\\nKate: It's so expensive! But they say, it's Tel Aviv... Tomorrow we are going to Jerusalem.\\r\\nMary: I've heard Israel is expensive, Monica was there on vacation last year, she complained about how pricey it is. Are you going to the Dead Sea before it dies? ahahahha\\r\\nKate: ahahhaha yup, in few days.\"\n",
      "Generated Summary: Mary and Kate are going to the beach tomorrow. Monica was there on vacation last year. She's going to the Dead Sea in few days.\n",
      "Ground Truth Summary: b'Mary and Kate discuss how expensive Israel is. Kate is in Tel Aviv now, planning to travel to Jerusalem tomorrow, and to the Dead Sea few days later.'\n",
      "=============================\n",
      "Dialogue: b\"Giny: do we have rice?\\r\\nRiley: nope, it's finished\\r\\nGiny: fuck!\\r\\nGiny: ok, I'll buy\"\n",
      "Generated Summary: Riley will buy rice. \n",
      "Ground Truth Summary: b\"Giny and Riley don't have any rice left. Giny will buy some.\"\n",
      "=============================\n",
      "Dialogue: b\"Jude: i'll be in warsaw at the beginning of december so we could meet again\\r\\nLeon: !!!\\r\\nLeon: at the beginning means...?\\r\\nLeon: cuz I won't be here during the first weekend\\r\\nJude: 10\\r\\nJude: but i think it's a monday, so never mind i guess :D\\r\\nLeon: yeah monday doesn't really work for me :D\\r\\nLeon: :<\\r\\nJude: oh well next time :d\\r\\nLeon: yeah...!\"\n",
      "Generated Summary: Jude won't be in warsaw during the 1st weekend of December. Leon and Jude will meet on Sunday.\n",
      "Ground Truth Summary: b'Jude is coming to Warsaw on the 10th of December and wants to see Leon. Leon has no time.'\n",
      "=============================\n"
     ]
    }
   ],
   "source": [
    "for dialogue, generated_summary, ground_truth_summary in zip(\n",
    "    dialogues[:5], generated_summaries[:5], ground_truth_summaries[:5]\n",
    "):\n",
    "    print(\"Dialogue:\", dialogue)\n",
    "    print(\"Generated Summary:\", generated_summary)\n",
    "    print(\"Ground Truth Summary:\", ground_truth_summary)\n",
    "    print(\"=============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
