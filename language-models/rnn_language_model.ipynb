{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchtext\n",
    "torchtext.disable_torchtext_deprecation_warning()\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence  # Import pad_sequence function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 75024.94 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 703916.88 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 442944.14 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 36718\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the WikiText-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    # Simple tokenizer that splits on non-alphabetic characters\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Build vocabulary\n",
    "counter = Counter()\n",
    "for line in dataset['train']['text']:\n",
    "    counter.update(tokenize(line))\n",
    "\n",
    "# Define vocabulary size and special tokens\n",
    "vocab_size = 10000\n",
    "special_tokens = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "vocab = {word: idx + len(special_tokens) for idx, (word, _) in enumerate(counter.most_common(vocab_size - len(special_tokens)))}\n",
    "for idx, token in enumerate(special_tokens):\n",
    "    vocab[token] = idx\n",
    "\n",
    "# Inverse vocabulary for decoding\n",
    "inv_vocab = {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Encode function\n",
    "def encode(text):\n",
    "    tokens = tokenize(text)\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "\n",
    "# Add special tokens to each sentence\n",
    "def add_special_tokens(encoded_text):\n",
    "    return [vocab['<bos>']] + encoded_text + [vocab['<eos>']]\n",
    "\n",
    "# Prepare dataset\n",
    "def prepare_dataset(split):\n",
    "    encoded_texts = [add_special_tokens(encode(line)) for line in dataset[split]['text']]\n",
    "    return encoded_texts\n",
    "\n",
    "train_data = prepare_dataset('train')\n",
    "valid_data = prepare_dataset('validation')\n",
    "test_data = prepare_dataset('test')\n",
    "\n",
    "# Data collate function for DataLoader\n",
    "def collate_fn(batch):\n",
    "    batch = [torch.tensor(item) for item in batch]\n",
    "    batch = pad_sequence(batch, batch_first=True, padding_value=vocab['<pad>'])\n",
    "    inputs = batch[:, :-1]\n",
    "    targets = batch[:, 1:]\n",
    "    return inputs, targets\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(RNNLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=vocab['<pad>'])\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        output, hidden = self.lstm(x, hidden)\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new_zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size),\n",
    "                weight.new_zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 128      # Size of word embeddings\n",
    "hidden_size = 256     # Number of features in the hidden state of the RNN\n",
    "num_layers = 2        # Number of recurrent layers (e.g., LSTM layers)\n",
    "num_epochs = 10       # Number of training epochs\n",
    "learning_rate = 0.001 # Learning rate for the optimizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = RNNLanguageModel(\n",
    "    vocab_size=len(vocab),   # Size of the vocabulary\n",
    "    embed_size=embed_size,   # Embedding size\n",
    "    hidden_size=hidden_size, # Hidden state size\n",
    "    num_layers=num_layers,   # Number of LSTM layers\n",
    "    dropout=0.5              # Dropout rate\n",
    ").to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>']) # Ignore padding in loss calculation\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Adam optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shreyak_rekshda/ai-ml-test/custom_language_models/wandb/run-20250219_084708-uu1anx5r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shreyakchakraborty/rnn-language-model/runs/uu1anx5r' target=\"_blank\">ethereal-violet-2</a></strong> to <a href='https://wandb.ai/shreyakchakraborty/rnn-language-model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shreyakchakraborty/rnn-language-model' target=\"_blank\">https://wandb.ai/shreyakchakraborty/rnn-language-model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shreyakchakraborty/rnn-language-model/runs/uu1anx5r' target=\"_blank\">https://wandb.ai/shreyakchakraborty/rnn-language-model/runs/uu1anx5r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(\n",
    "    project=\"rnn-language-model\",\n",
    "    config={\n",
    "        \"embed_size\": embed_size,\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_epochs\": num_epochs,\n",
    "    }\n",
    ")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# Function to train the model with logging and dynamic hidden state initialization.\n",
    "# The function now accepts a `start_epoch` parameter.\n",
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs, start_epoch=1, log_interval=100):\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = len(train_loader)\n",
    "        \n",
    "        # Initialize hidden state for the first batch; will update it dynamically later.\n",
    "        hidden = None\n",
    "\n",
    "        # Use tqdm for progress bar\n",
    "        pbar = tqdm(enumerate(train_loader), total=num_batches, desc=f\"Epoch {epoch}\")\n",
    "        for batch_idx, (inputs, targets) in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            current_batch_size = inputs.size(0)\n",
    "            \n",
    "            # Initialize or update hidden state dynamically based on current batch size.\n",
    "            if hidden is None or hidden[0].size(1) != current_batch_size:\n",
    "                hidden = model.init_hidden(current_batch_size)\n",
    "            else:\n",
    "                # Detach hidden state to prevent backpropagating through the entire history\n",
    "                hidden = tuple([h.detach() for h in hidden])\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            loss = criterion(output.view(-1, len(vocab)), targets.view(-1))\n",
    "            loss.backward()  # Backward pass\n",
    "            \n",
    "            # Clip gradients to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()  # Update parameters\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Logging every `log_interval` steps\n",
    "            if (batch_idx + 1) % log_interval == 0:\n",
    "                avg_loss = total_loss / log_interval\n",
    "                wandb.log({\"Training Loss\": avg_loss, \"Epoch\": epoch, \"Batch\": batch_idx})\n",
    "                pbar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "                total_loss = 0  # Reset loss tracker\n",
    "\n",
    "        # Validate after each epoch\n",
    "        val_loss = evaluate_model(model, valid_loader, criterion)\n",
    "        wandb.log({\"Validation Loss\": val_loss, \"Epoch\": epoch})\n",
    "        print(f\"Epoch {epoch}: Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        #Save a checkpoint after 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'validation_loss': val_loss\n",
    "            }\n",
    "            torch.save(checkpoint, f'checkpoint_epoch{epoch}.pth')\n",
    "            print(f\"Checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# Function to evaluate the model remains unchanged.\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0\n",
    "    num_batches = len(data_loader)\n",
    "    hidden = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            current_batch_size = inputs.size(0)\n",
    "            if hidden is None or hidden[0].size(1) != current_batch_size:\n",
    "                hidden = model.init_hidden(current_batch_size)\n",
    "            else:\n",
    "                hidden = tuple([h.detach() for h in hidden])\n",
    "            output, hidden = model(inputs, hidden)\n",
    "            loss = criterion(output.view(-1, len(vocab)), targets.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    model.train()  # Switch back to training mode\n",
    "    return total_loss / num_batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1148/1148 [00:41<00:00, 27.87it/s, loss=5.8464]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Validation Loss: 5.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1148/1148 [00:40<00:00, 28.45it/s, loss=5.7392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Validation Loss: 5.4788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1148/1148 [00:40<00:00, 28.34it/s, loss=5.6471]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Validation Loss: 5.3878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1148/1148 [00:40<00:00, 28.12it/s, loss=5.5780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Validation Loss: 5.3182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1148/1148 [00:40<00:00, 28.57it/s, loss=5.4934]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Validation Loss: 5.2707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1148/1148 [00:40<00:00, 28.37it/s, loss=5.4207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Validation Loss: 5.2227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1148/1148 [00:40<00:00, 28.20it/s, loss=5.4009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Validation Loss: 5.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1148/1148 [00:40<00:00, 28.29it/s, loss=5.3519]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Validation Loss: 5.1531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1148/1148 [00:40<00:00, 28.29it/s, loss=5.3212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Validation Loss: 5.1305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 1148/1148 [00:40<00:00, 28.41it/s, loss=5.2686]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Validation Loss: 5.1090\n"
     ]
    }
   ],
   "source": [
    "# Start training (from beginning - DO NOT RUN THIS IF RESUMING TRAINING)\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=num_epochs, start_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 1148/1148 [00:40<00:00, 28.31it/s, loss=5.2538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Validation Loss: 5.0909\n",
      "Checkpoint saved at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 1148/1148 [00:40<00:00, 28.37it/s, loss=5.2273]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Validation Loss: 5.0722\n",
      "Checkpoint saved at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 1148/1148 [00:40<00:00, 28.11it/s, loss=5.2133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Validation Loss: 5.0566\n",
      "Checkpoint saved at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 1148/1148 [00:40<00:00, 28.30it/s, loss=5.1836]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Validation Loss: 5.0445\n",
      "Checkpoint saved at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 1148/1148 [00:40<00:00, 28.25it/s, loss=5.1602]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Validation Loss: 5.0293\n",
      "Checkpoint saved at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 1148/1148 [00:40<00:00, 28.25it/s, loss=5.1571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Validation Loss: 5.0215\n",
      "Checkpoint saved at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 1148/1148 [00:40<00:00, 28.33it/s, loss=5.1071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Validation Loss: 5.0110\n",
      "Checkpoint saved at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 1148/1148 [00:40<00:00, 28.39it/s, loss=5.1229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Validation Loss: 5.0012\n",
      "Checkpoint saved at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1148/1148 [00:40<00:00, 28.25it/s, loss=5.0664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Validation Loss: 4.9959\n",
      "Checkpoint saved at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 1148/1148 [00:40<00:00, 28.44it/s, loss=5.0547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Validation Loss: 4.9926\n",
      "Checkpoint saved at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 1148/1148 [00:40<00:00, 28.34it/s, loss=5.0620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Validation Loss: 4.9795\n",
      "Checkpoint saved at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 1148/1148 [00:40<00:00, 28.41it/s, loss=5.0521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Validation Loss: 4.9746\n",
      "Checkpoint saved at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 1148/1148 [00:40<00:00, 28.25it/s, loss=5.0224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Validation Loss: 4.9711\n",
      "Checkpoint saved at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 1148/1148 [00:40<00:00, 28.45it/s, loss=5.0168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Validation Loss: 4.9674\n",
      "Checkpoint saved at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 1148/1148 [00:40<00:00, 28.25it/s, loss=5.0180]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Validation Loss: 4.9589\n",
      "Checkpoint saved at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 1148/1148 [00:40<00:00, 28.12it/s, loss=5.0203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Validation Loss: 4.9570\n",
      "Checkpoint saved at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 1148/1148 [00:40<00:00, 28.21it/s, loss=5.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Validation Loss: 4.9502\n",
      "Checkpoint saved at epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 1148/1148 [00:40<00:00, 28.45it/s, loss=4.9729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Validation Loss: 4.9453\n",
      "Checkpoint saved at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 1148/1148 [00:40<00:00, 28.25it/s, loss=4.9762]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Validation Loss: 4.9406\n",
      "Checkpoint saved at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 1148/1148 [00:40<00:00, 28.39it/s, loss=4.9610]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Validation Loss: 4.9417\n",
      "Checkpoint saved at epoch 30\n"
     ]
    }
   ],
   "source": [
    "# Load the previous checkpoint\n",
    "checkpoint = torch.load('checkpoint_epoch10.pth', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# Set the start epoch to one after the checkpoint's epoch\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "print(f\"Resuming training from epoch {start_epoch}\")\n",
    "\n",
    "# Specify the additional number of epochs you want to train for\n",
    "additional_epochs = 20\n",
    "\n",
    "# Resume training using the same train_model function\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=additional_epochs, start_epoch=start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Batch</td><td>▂▃▅▆▄▄▆▇▁▅▇▂▅▂▃▂█▃▄▆▁▂▄██▄▇▇▃▅▄▁▂▄▇▆▇▂▅▇</td></tr><tr><td>Epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>Training Loss</td><td>███▇▇▆▇▇▆▆▆▅▆▆▆▅▅▄▅▅▄▄▃▄▄▄▃▄▃▃▂▂▃▂▂▂▁▂▂▂</td></tr><tr><td>Validation Loss</td><td>█▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Batch</td><td>1099</td></tr><tr><td>Epoch</td><td>30</td></tr><tr><td>Training Loss</td><td>4.96097</td></tr><tr><td>Validation Loss</td><td>4.94166</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-violet-2</strong> at: <a href='https://wandb.ai/shreyakchakraborty/rnn-language-model/runs/uu1anx5r' target=\"_blank\">https://wandb.ai/shreyakchakraborty/rnn-language-model/runs/uu1anx5r</a><br> View project at: <a href='https://wandb.ai/shreyakchakraborty/rnn-language-model' target=\"_blank\">https://wandb.ai/shreyakchakraborty/rnn-language-model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250219_084708-uu1anx5r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model to disk\n",
    "saved_model_name = 'rnn_language_model_30epochs.pth'\n",
    "torch.save(model.state_dict(), saved_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(saved_model_name, map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, input_text, vocab, inv_vocab, top_k=5):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    # Tokenize the input text\n",
    "    tokens = tokenize(input_text)\n",
    "    # Convert tokens to indices, using <unk> for unseen words\n",
    "    input_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    # Convert to tensor and add batch dimension (1, sequence_length)\n",
    "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize hidden state with batch size 1\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass: get output predictions and update hidden state\n",
    "        output, hidden = model(input_tensor, hidden)\n",
    "    \n",
    "    # Get logits for the last token in the sequence\n",
    "    logits = output[0, -1]  # Shape: [vocab_size]\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = torch.softmax(logits, dim=0)\n",
    "    # Get the indices of the top_k words with highest probability\n",
    "    top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "    # Map indices to words using the inverse vocabulary\n",
    "    top_words = [inv_vocab[idx.item()] for idx in top_indices]\n",
    "    \n",
    "    return top_words\n",
    "\n",
    "def generate_text_sample(model, prompt, vocab, inv_vocab, num_tokens, top_k=5, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate text by predicting and appending num_tokens to the prompt using sampling.\n",
    "    \"\"\"\n",
    "    generated_text = prompt\n",
    "    \n",
    "    for _ in range(num_tokens):\n",
    "        # Get predictions from the existing function\n",
    "        predictions = predict_next_word(model, generated_text, vocab, inv_vocab, top_k=top_k)\n",
    "        \n",
    "        # Instead of choosing the top prediction, sample from top-k.\n",
    "        # First, get the raw output logits from the model:\n",
    "        tokens = tokenize(generated_text)\n",
    "        input_ids = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "        input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
    "        hidden = model.init_hidden(1)\n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "        logits = output[0, -1] / temperature  # adjust logits by temperature\n",
    "        probabilities = torch.softmax(logits, dim=0)\n",
    "        \n",
    "        # Get top-k probabilities and indices\n",
    "        top_probs, top_indices = torch.topk(probabilities, top_k)\n",
    "        \n",
    "        # Sample from the top-k tokens according to their probabilities\n",
    "        top_probs = top_probs / torch.sum(top_probs)  # normalize\n",
    "        sampled_index = torch.multinomial(top_probs, 1).item()\n",
    "        next_word_idx = top_indices[sampled_index].item()\n",
    "        next_word = inv_vocab.get(next_word_idx, \"<unk>\")\n",
    "        \n",
    "        generated_text += \" \" + next_word\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "def generate_text_naive(prompt,num_tokens=100):\n",
    "    generated_text = prompt\n",
    "    for i in range(num_tokens):\n",
    "        predicted_words = predict_next_word(model, generated_text, vocab, inv_vocab, top_k=5)\n",
    "        next_word = predicted_words[0]\n",
    "        if next_word == \"<unk>\":\n",
    "            next_word = predicted_words[2]\n",
    "        \n",
    "        generated_text += next_word + \" \"\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:  I am a man. in the world s last day of the year the album was released in the united states and the song s release of the album s first album album in the united states and the song was released in the united states and the song s release of the album \n"
     ]
    }
   ],
   "source": [
    "prompt = \"I am a man. \"\n",
    "generated_text = generate_text_naive(prompt,50)\n",
    "print(\"Generated text: \",generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: I am a man. I can be a very good \n",
      "Next word predictions: ['<unk>', 'and', 'of', 'thing', 'to']\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "prompt = \"I am a man. I can be a very good \"\n",
    "predicted_words = predict_next_word(model, prompt, vocab, inv_vocab, top_k=5)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Next word predictions:\", predicted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Hello, this is  the most common <unk> in <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "prompt = \"Hello, this is \"\n",
    "generated = generate_text_sample(model, prompt, vocab, inv_vocab, num_tokens=20, top_k=5, temperature=.5)\n",
    "print(\"Generated Text:\")\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
