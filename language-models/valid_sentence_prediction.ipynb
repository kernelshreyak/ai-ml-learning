{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning BERT to predict valid english sentence"
      ],
      "metadata": {
        "id": "sVpT_pAiePfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "IxP3WuqLY8bb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from transformers import TrainerCallback, PrinterCallback\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SentenceDataset(Dataset):\n",
        "    \"\"\"Custom dataset for sentence validity classification.\"\"\"\n",
        "    def __init__(self, texts, labels, processor, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc = self.processor(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        item = {k: v.squeeze() for k, v in enc.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "\n",
        "def train_classifier(\n",
        "    train_texts,\n",
        "    train_labels,\n",
        "    val_texts=None,\n",
        "    val_labels=None,\n",
        "    model_name: str = 'distilbert-base-uncased',\n",
        "    output_dir: str = './sentence_classifier',\n",
        "    epochs: int = 3,\n",
        "    batch_size: int = 16,\n",
        "    learning_rate: float = 2e-5\n",
        "):\n",
        "    \"\"\"\n",
        "    Fine-tunes a pretrained transformer for binary sentence validity classification.\n",
        "\n",
        "    Returns the trained Trainer instance.\n",
        "    \"\"\"\n",
        "    processor = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2\n",
        "    )\n",
        "\n",
        "    train_dataset = SentenceDataset(train_texts, train_labels, processor)\n",
        "    eval_dataset = None\n",
        "    if val_texts is not None and val_labels is not None:\n",
        "        eval_dataset = SentenceDataset(val_texts, val_labels, processor)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        logging_steps=100,\n",
        "        save_steps=500,\n",
        "        eval_steps=500,\n",
        "        logging_dir=f'{output_dir}/logs',\n",
        "        report_to='wandb'  # or 'wandb'\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=processor,  # Correct parameter name\n",
        "        callbacks=[PrinterCallback]\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer.train()\n",
        "    # Manual evaluation if validation data provided\n",
        "    if eval_dataset is not None:\n",
        "        trainer.evaluate(eval_dataset)\n",
        "    return trainer\n",
        "\n",
        "\n",
        "def predict_validity(texts, trainer):\n",
        "    \"\"\"\n",
        "    Predicts validity (0=invalid, 1=valid) for a list of texts using a trained Trainer.\n",
        "    \"\"\"\n",
        "    class _Wrapper(Dataset):\n",
        "        def __init__(self, texts, processor, max_length=128):\n",
        "            self.texts = texts\n",
        "            self.processor = processor\n",
        "            self.max_length = max_length\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.texts)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            enc = self.processor(\n",
        "                self.texts[idx],\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=self.max_length,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            return {k: v.squeeze() for k, v in enc.items()}\n",
        "\n",
        "    pred_dataset = _Wrapper(texts, trainer.processing_class)\n",
        "    outputs = trainer.predict(pred_dataset)\n",
        "    preds = np.argmax(outputs.predictions, axis=1)\n",
        "    return preds.tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "p1LLAVVqdbjo"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_texts = [\n",
        "    \"I love reading books.\",\n",
        "    \"Hello!\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"How are you?\",\n",
        "    \"Good morning.\",\n",
        "    \"Absolutely!\",\n",
        "    \"Sure.\",\n",
        "    \"Thanks a lot.\",\n",
        "    \"Yes\",\n",
        "    \"Nice work\",\n",
        "    \"Okay\",\n",
        "    \"This is interesting.\",\n",
        "    \"What time is it?\",\n",
        "    \"Running fast is fun.\",\n",
        "    \"He plays the piano.\",\n",
        "    \"good\",\n",
        "    \"Wonderful day\",\n",
        "    \"Lunch at noon\",\n",
        "    \"Be careful!\",\n",
        "    \"Do it now\"\n",
        "]\n",
        "\n",
        "invalid_texts = [\n",
        "    \"asdfgh\",\n",
        "    \"qwertyuiop\",\n",
        "    \"loremipsum\",\n",
        "    \"12345\",\n",
        "    \"!!!???\",\n",
        "    \"___--\",\n",
        "    \"afg78gdf\",\n",
        "    \"hjkl hjkl\",\n",
        "    \"blahblahblah\",\n",
        "    \"zxcvb asdfg\",\n",
        "    \".....\",\n",
        "    \",,,,,\",\n",
        "    \"@#$%^&*()\",\n",
        "    \"rrrrrrrrrrrrrrr\",\n",
        "    \"wtrhysd\",\n",
        "    \"ðŸ‘¾ðŸ‘¾ðŸ‘¾\",\n",
        "    \"dsklfjsd\",\n",
        "    \"yt!op\",\n",
        "    \"xxxxxxxxxx\",\n",
        "    \"999999\"\n",
        "]\n",
        "\n",
        "train_texts = valid_texts + invalid_texts\n",
        "train_labels = [1] * len(valid_texts) + [0] * len(invalid_texts)\n"
      ],
      "metadata": {
        "id": "W3FEfQotdfSS"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_texts = [\"Hello world!\", \"ghjk lkjh hjkl\"]\n",
        "val_labels = [1, 0]\n",
        "\n",
        "trainer = train_classifier(\n",
        "    train_texts,\n",
        "    train_labels,\n",
        "    val_texts,\n",
        "    val_labels,\n",
        "    model_name='distilbert-base-uncased',\n",
        "    epochs=20\n",
        ")\n",
        "\n",
        "test_sentences = [\n",
        "    \"This is a test.\",\n",
        "    \"qwerty 12345\"\n",
        "]\n",
        "preds = predict_validity(test_sentences, trainer)\n",
        "for sent, p in zip(test_sentences, preds):\n",
        "    print(f\"\\\"{sent}\\\": {'Valid' if p == 1 else 'Invalid'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "TRWOQW6HdgRT",
        "outputId": "b44bae2c-2ab1-43d1-cd8e-50e5ff0e8715"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-40-44a8a41677a6>:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 00:11, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train_runtime': 11.2985, 'train_samples_per_second': 70.806, 'train_steps_per_second': 5.31, 'train_loss': 0.26260852813720703, 'epoch': 20.0}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.04613985866308212, 'eval_runtime': 0.0148, 'eval_samples_per_second': 134.781, 'eval_steps_per_second': 67.39, 'epoch': 20.0}\n",
            "\"This is a test.\": Valid\n",
            "\"qwerty 12345\": Invalid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_validity([\"fasfsaffasfasf\"], trainer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IRONwHmbZFDt",
        "outputId": "9b2cd84b-4630-440c-8cc9-5d2cf99a3ab4"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict_validity([\"poison\"], trainer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VNczo_hQb21O",
        "outputId": "762b9354-7d05-43aa-d5c9-54724dc7e338"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P3mlL8L4eB9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}